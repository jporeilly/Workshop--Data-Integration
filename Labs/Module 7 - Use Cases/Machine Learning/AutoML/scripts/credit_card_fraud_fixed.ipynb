{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q0iaOYCDIvu1"
      },
      "source": [
        "# TPOT - Credit Card Fraud Detection (Fixed for TPOT 2.x)\n",
        "\n",
        "This notebook demonstrates automated machine learning for credit card fraud detection using TPOT 2.x with best practices:\n",
        "- Fixed for TPOT 2.x API compatibility\n",
        "- Proper handling of imbalanced data\n",
        "- Comprehensive evaluation metrics\n",
        "- Data validation and exploration\n",
        "- Model comparison and visualization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7bR4IYms0VYr"
      },
      "source": [
        "> ### Install Required Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3XuglahR0dlv"
      },
      "outputs": [],
      "source": [
        "# Install TPOT and additional libraries for visualization\n",
        "!pip install tpot matplotlib seaborn scikit-learn imbalanced-learn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dn_t1SOrJwOC"
      },
      "source": [
        "> ### Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gBxSPYHsJ7Hj"
      },
      "outputs": [],
      "source": [
        "# Core libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from datetime import datetime\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# TPOT and sklearn\n",
        "from tpot import TPOTClassifier\n",
        "from sklearn import preprocessing\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.metrics import (\n",
        "    classification_report, \n",
        "    confusion_matrix, \n",
        "    roc_auc_score, \n",
        "    roc_curve,\n",
        "    precision_recall_curve,\n",
        "    f1_score,\n",
        "    accuracy_score,\n",
        "    precision_score,\n",
        "    recall_score\n",
        ")\n",
        "\n",
        "# For handling imbalanced data\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from collections import Counter\n",
        "\n",
        "# Set plotting style\n",
        "sns.set_style('whitegrid')\n",
        "plt.rcParams['figure.figsize'] = (10, 6)\n",
        "\n",
        "# Check TPOT version\n",
        "import tpot\n",
        "print(f\"TPOT version: {tpot.__version__}\")\n",
        "print(\"All libraries imported successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rogF_vGbLjWx"
      },
      "source": [
        "> ### Import Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aHkRl7NdOYfs"
      },
      "outputs": [],
      "source": [
        "# Upload dataset from local drive\n",
        "from google.colab import files\n",
        "uploaded = files.upload()\n",
        "for fn in uploaded.keys():\n",
        "    print(f'User uploaded file \"{fn}\" with length {len(uploaded[fn])} bytes')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7AqVkigoO7Yc"
      },
      "outputs": [],
      "source": [
        "# Load the TPOT.csv dataset\n",
        "dataset = pd.read_csv('TPOT.csv', sep=';', header=None)\n",
        "\n",
        "# Add descriptive column headers\n",
        "dataset.columns = [\n",
        "    'first_time_customer',\n",
        "    'order_dollar_amount',\n",
        "    'num_items',\n",
        "    'age',\n",
        "    'web_order',\n",
        "    'total_transactions_to_date',\n",
        "    'hour_of_day',\n",
        "    'billing_shipping_zip_equal',\n",
        "    'reported_as_fraud_historic'\n",
        "]\n",
        "\n",
        "print(f\"Dataset loaded: {dataset.shape[0]} rows, {dataset.shape[1]} columns\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3NURFI_GMUHl"
      },
      "source": [
        "> ### Exploratory Data Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VuTFh1I8M25U"
      },
      "outputs": [],
      "source": [
        "# Display first few rows\n",
        "print(\"First 5 rows of the dataset:\")\n",
        "display(dataset.head())\n",
        "\n",
        "# Dataset information\n",
        "print(\"\\nDataset Info:\")\n",
        "dataset.info()\n",
        "\n",
        "# Statistical summary\n",
        "print(\"\\nStatistical Summary:\")\n",
        "display(dataset.describe())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check for missing values\n",
        "print(\"Missing values per column:\")\n",
        "print(dataset.isnull().sum())\n",
        "\n",
        "# Check data types\n",
        "print(\"\\nData types:\")\n",
        "print(dataset.dtypes)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> #### Class Balance Analysis (Critical for Fraud Detection!)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Analyze class distribution\n",
        "fraud_counts = dataset['reported_as_fraud_historic'].value_counts()\n",
        "fraud_ratio = fraud_counts[1] / fraud_counts[0] if len(fraud_counts) > 1 else 0\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"CLASS DISTRIBUTION ANALYSIS\")\n",
        "print(\"=\"*50)\n",
        "print(f\"Non-Fraud (0): {fraud_counts.get(0, 0):,} ({fraud_counts.get(0, 0)/len(dataset)*100:.2f}%)\")\n",
        "print(f\"Fraud (1):     {fraud_counts.get(1, 0):,} ({fraud_counts.get(1, 0)/len(dataset)*100:.2f}%)\")\n",
        "print(f\"Fraud Ratio:   1:{1/fraud_ratio:.1f}\" if fraud_ratio > 0 else \"No fraud cases found\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# Visualize class distribution\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# Bar chart\n",
        "fraud_counts.plot(kind='bar', ax=axes[0], color=['#2ecc71', '#e74c3c'])\n",
        "axes[0].set_title('Class Distribution', fontsize=14, fontweight='bold')\n",
        "axes[0].set_xlabel('Class (0=Normal, 1=Fraud)')\n",
        "axes[0].set_ylabel('Count')\n",
        "axes[0].set_xticklabels(['Normal', 'Fraud'], rotation=0)\n",
        "for i, v in enumerate(fraud_counts):\n",
        "    axes[0].text(i, v + 50, str(v), ha='center', fontweight='bold')\n",
        "\n",
        "# Pie chart\n",
        "axes[1].pie(fraud_counts, labels=['Normal', 'Fraud'], autopct='%1.1f%%',\n",
        "            colors=['#2ecc71', '#e74c3c'], startangle=90)\n",
        "axes[1].set_title('Class Distribution (%)', fontsize=14, fontweight='bold')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Warning for imbalanced data\n",
        "if fraud_ratio < 0.1:\n",
        "    print(\"\\nâš ï¸  WARNING: Highly imbalanced dataset detected!\")\n",
        "    print(\"   Consider using SMOTE or adjusting class weights.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> #### Feature Distribution Visualization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize feature distributions by fraud status\n",
        "numeric_features = [\n",
        "    'order_dollar_amount', 'num_items', 'age', \n",
        "    'total_transactions_to_date', 'hour_of_day'\n",
        "]\n",
        "\n",
        "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
        "axes = axes.flatten()\n",
        "\n",
        "for idx, feature in enumerate(numeric_features):\n",
        "    for fraud_label in [0, 1]:\n",
        "        data = dataset[dataset['reported_as_fraud_historic'] == fraud_label][feature]\n",
        "        axes[idx].hist(data, alpha=0.6, bins=30, \n",
        "                      label=f'Fraud={fraud_label}',\n",
        "                      color='#e74c3c' if fraud_label == 1 else '#2ecc71')\n",
        "    axes[idx].set_title(f'{feature.replace(\"_\", \" \").title()}', fontweight='bold')\n",
        "    axes[idx].legend()\n",
        "    axes[idx].set_xlabel(feature.replace('_', ' ').title())\n",
        "    axes[idx].set_ylabel('Frequency')\n",
        "\n",
        "# Remove extra subplot\n",
        "fig.delaxes(axes[-1])\n",
        "\n",
        "plt.suptitle('Feature Distributions by Fraud Status', fontsize=16, fontweight='bold', y=1.02)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> #### Correlation Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compute correlation matrix\n",
        "plt.figure(figsize=(12, 10))\n",
        "correlation_matrix = dataset.corr()\n",
        "sns.heatmap(correlation_matrix, annot=True, fmt='.2f', cmap='coolwarm', \n",
        "            center=0, square=True, linewidths=1, cbar_kws={\"shrink\": 0.8})\n",
        "plt.title('Feature Correlation Matrix', fontsize=16, fontweight='bold', pad=20)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Show correlations with target variable\n",
        "print(\"\\nCorrelation with Fraud (Target Variable):\")\n",
        "target_corr = correlation_matrix['reported_as_fraud_historic'].sort_values(ascending=False)\n",
        "print(target_corr)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ov1Z26Nr5CpZ"
      },
      "source": [
        "> ### Data Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K2PYKxTj5GmS"
      },
      "outputs": [],
      "source": [
        "# Separate features and target\n",
        "X = dataset.iloc[:, :-1].values\n",
        "y = dataset.iloc[:, -1].values\n",
        "\n",
        "print(f\"Features shape: {X.shape}\")\n",
        "print(f\"Target shape: {y.shape}\")\n",
        "\n",
        "# Scale features using MinMaxScaler\n",
        "print(\"\\nScaling features...\")\n",
        "scaler = preprocessing.MinMaxScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "print(\"Feature scaling completed!\")\n",
        "print(f\"Scaled features range: [{X_scaled.min():.3f}, {X_scaled.max():.3f}]\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TAXfHB901vhj"
      },
      "source": [
        "> ### Train/Test Split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NMpR33la6nVk"
      },
      "outputs": [],
      "source": [
        "# Split dataset: 75% training, 25% testing (standard split)\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X_scaled, y, \n",
        "    test_size=0.25,      # Use 25% for testing\n",
        "    random_state=42,     # For reproducibility\n",
        "    stratify=y           # Maintain class distribution in splits\n",
        ")\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"TRAIN/TEST SPLIT\")\n",
        "print(\"=\"*50)\n",
        "print(f\"Training set:   {X_train.shape[0]:,} samples ({X_train.shape[0]/len(X_scaled)*100:.1f}%)\")\n",
        "print(f\"Test set:       {X_test.shape[0]:,} samples ({X_test.shape[0]/len(X_scaled)*100:.1f}%)\")\n",
        "print(f\"\\nTraining fraud cases: {sum(y_train)} ({sum(y_train)/len(y_train)*100:.2f}%)\")\n",
        "print(f\"Test fraud cases:     {sum(y_test)} ({sum(y_test)/len(y_test)*100:.2f}%)\")\n",
        "print(\"=\"*50)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> ### Handle Class Imbalance (Optional but Recommended)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Option: Apply SMOTE to balance training data\n",
        "\n",
        "USE_SMOTE = True  # Set to True to enable SMOTE\n",
        "\n",
        "if USE_SMOTE:\n",
        "    print(\"\\nApplying SMOTE to balance training data...\")\n",
        "    print(f\"Before SMOTE: {Counter(y_train)}\")\n",
        "    \n",
        "    smote = SMOTE(random_state=42)\n",
        "    X_train_balanced, y_train_balanced = smote.fit_resample(X_train, y_train)\n",
        "    \n",
        "    print(f\"After SMOTE:  {Counter(y_train_balanced)}\")\n",
        "    print(f\"\\nTraining samples increased from {len(y_train):,} to {len(y_train_balanced):,}\")\n",
        "    \n",
        "    # Use balanced data for training\n",
        "    X_train_final = X_train_balanced\n",
        "    y_train_final = y_train_balanced\n",
        "else:\n",
        "    print(\"\\nUsing original imbalanced training data\")\n",
        "    X_train_final = X_train\n",
        "    y_train_final = y_train"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IxXkBXkK7Scu"
      },
      "source": [
        "> ### TPOT Classifier Configuration (TPOT 2.x Compatible)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configuration - Adjust these parameters\n",
        "GENERATIONS = 5        # Number of pipeline iterations (3-10 recommended)\n",
        "POPULATION_SIZE = 20   # Number of pipelines per generation (10-50 recommended)\n",
        "CV_FOLDS = 5           # Cross-validation folds\n",
        "SCORING = 'roc_auc'    # Scoring metric for fraud detection\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"TPOT CONFIGURATION\")\n",
        "print(\"=\"*50)\n",
        "print(f\"Generations:      {GENERATIONS}\")\n",
        "print(f\"Population Size:  {POPULATION_SIZE}\")\n",
        "print(f\"CV Folds:         {CV_FOLDS}\")\n",
        "print(f\"Scoring Metric:   {SCORING}\")\n",
        "print(f\"Use SMOTE:        {USE_SMOTE}\")\n",
        "print(\"=\"*50)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> ### Run TPOT Training (Fixed for TPOT 2.x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "61rEjbvn7xB6"
      },
      "outputs": [],
      "source": [
        "# Initialize TPOT Classifier (TPOT 2.x compatible)\n",
        "print(\"\\nInitializing TPOT Classifier...\")\n",
        "\n",
        "tpot = TPOTClassifier(\n",
        "    generations=GENERATIONS,\n",
        "    population_size=POPULATION_SIZE,\n",
        "    cv=CV_FOLDS,\n",
        "    scoring=SCORING,\n",
        "    random_state=42,\n",
        "    n_jobs=-1,              # Use all CPU cores\n",
        "    # Note: 'verbosity' is removed - TPOT 2.x uses logging instead\n",
        "    # Note: 'config_dict' and 'early_stop' may not be available in TPOT 2.x\n",
        ")\n",
        "\n",
        "# Record start time\n",
        "start_time = datetime.now()\n",
        "print(f\"\\nTraining started at: {start_time.strftime('%Y-%m-%d %H:%M:%S')}\")\n",
        "print(\"\\nThis may take several minutes...\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# Train TPOT\n",
        "tpot.fit(X_train_final, y_train_final)\n",
        "\n",
        "# Record end time\n",
        "end_time = datetime.now()\n",
        "training_duration = (end_time - start_time).total_seconds()\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"TRAINING COMPLETED!\")\n",
        "print(\"=\"*50)\n",
        "print(f\"Training time: {training_duration:.2f} seconds ({training_duration/60:.2f} minutes)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> ### Model Evaluation - Comprehensive Metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get predictions\n",
        "y_pred = tpot.predict(X_test)\n",
        "y_pred_proba = tpot.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# Calculate metrics\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"MODEL PERFORMANCE METRICS\")\n",
        "print(\"=\"*60)\n",
        "print(f\"Accuracy:  {accuracy:.4f}  (Overall correctness)\")\n",
        "print(f\"Precision: {precision:.4f}  (Of predicted frauds, how many are actual frauds)\")\n",
        "print(f\"Recall:    {recall:.4f}  (Of actual frauds, how many did we catch)\")\n",
        "print(f\"F1 Score:  {f1:.4f}  (Harmonic mean of precision and recall)\")\n",
        "print(f\"ROC-AUC:   {roc_auc:.4f}  (Area under ROC curve)\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Detailed classification report\n",
        "print(\"\\nDetailed Classification Report:\")\n",
        "print(classification_report(y_test, y_pred, \n",
        "                          target_names=['Normal', 'Fraud'],\n",
        "                          digits=4))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> #### Confusion Matrix Visualization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Confusion matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
        "\n",
        "# Absolute values\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[0],\n",
        "            xticklabels=['Normal', 'Fraud'],\n",
        "            yticklabels=['Normal', 'Fraud'],\n",
        "            cbar_kws={'label': 'Count'})\n",
        "axes[0].set_title('Confusion Matrix (Counts)', fontsize=14, fontweight='bold')\n",
        "axes[0].set_ylabel('Actual', fontsize=12)\n",
        "axes[0].set_xlabel('Predicted', fontsize=12)\n",
        "\n",
        "# Add annotations\n",
        "axes[0].text(0.5, -0.15, f'True Negatives: {cm[0,0]}', \n",
        "             transform=axes[0].transAxes, ha='left', fontsize=10)\n",
        "axes[0].text(0.5, -0.20, f'False Positives: {cm[0,1]}', \n",
        "             transform=axes[0].transAxes, ha='left', fontsize=10, color='red')\n",
        "axes[0].text(0.5, -0.25, f'False Negatives: {cm[1,0]}', \n",
        "             transform=axes[0].transAxes, ha='left', fontsize=10, color='red')\n",
        "axes[0].text(0.5, -0.30, f'True Positives: {cm[1,1]}', \n",
        "             transform=axes[0].transAxes, ha='left', fontsize=10)\n",
        "\n",
        "# Percentage values\n",
        "cm_percent = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis] * 100\n",
        "sns.heatmap(cm_percent, annot=True, fmt='.2f', cmap='Greens', ax=axes[1],\n",
        "            xticklabels=['Normal', 'Fraud'],\n",
        "            yticklabels=['Normal', 'Fraud'],\n",
        "            cbar_kws={'label': 'Percentage'})\n",
        "axes[1].set_title('Confusion Matrix (Percentages)', fontsize=14, fontweight='bold')\n",
        "axes[1].set_ylabel('Actual', fontsize=12)\n",
        "axes[1].set_xlabel('Predicted', fontsize=12)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> #### ROC and Precision-Recall Curves"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Calculate curves\n",
        "fpr, tpr, roc_thresholds = roc_curve(y_test, y_pred_proba)\n",
        "precision_curve, recall_curve, pr_thresholds = precision_recall_curve(y_test, y_pred_proba)\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
        "\n",
        "# ROC Curve\n",
        "axes[0].plot(fpr, tpr, linewidth=2, label=f'ROC Curve (AUC = {roc_auc:.4f})')\n",
        "axes[0].plot([0, 1], [0, 1], 'k--', linewidth=1, label='Random Classifier')\n",
        "axes[0].set_xlabel('False Positive Rate', fontsize=12)\n",
        "axes[0].set_ylabel('True Positive Rate', fontsize=12)\n",
        "axes[0].set_title('ROC Curve', fontsize=14, fontweight='bold')\n",
        "axes[0].legend(loc='lower right', fontsize=11)\n",
        "axes[0].grid(True, alpha=0.3)\n",
        "\n",
        "# Precision-Recall Curve\n",
        "axes[1].plot(recall_curve, precision_curve, linewidth=2, \n",
        "             label=f'PR Curve (AUC = {np.trapz(precision_curve, recall_curve):.4f})')\n",
        "axes[1].axhline(y=sum(y_test)/len(y_test), color='k', linestyle='--', \n",
        "                linewidth=1, label='Baseline (Random)')\n",
        "axes[1].set_xlabel('Recall', fontsize=12)\n",
        "axes[1].set_ylabel('Precision', fontsize=12)\n",
        "axes[1].set_title('Precision-Recall Curve', fontsize=14, fontweight='bold')\n",
        "axes[1].legend(loc='lower left', fontsize=11)\n",
        "axes[1].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nðŸ’¡ For fraud detection:\")\n",
        "print(\"   - High RECALL is critical (catch as many frauds as possible)\")\n",
        "print(\"   - Balance with PRECISION to avoid too many false alarms\")\n",
        "print(\"   - ROC-AUC shows overall discriminative ability\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> #### Best Pipeline Details"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"BEST PIPELINE FOUND BY TPOT\")\n",
        "print(\"=\"*60)\n",
        "print(tpot.fitted_pipeline_)\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Get pipeline steps\n",
        "print(\"\\nPipeline Steps:\")\n",
        "for idx, (name, transformer) in enumerate(tpot.fitted_pipeline_.steps, 1):\n",
        "    print(f\"{idx}. {name}: {transformer.__class__.__name__}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> ### Export Results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6jAicg4DB-PD"
      },
      "source": [
        "> #### Export Pipeline as Python Script"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vpHMD0nOCfhn"
      },
      "outputs": [],
      "source": [
        "# Export the best pipeline as a Python script\n",
        "pipeline_filename = f'tpot_fraud_pipeline_{datetime.now().strftime(\"%Y%m%d_%H%M%S\")}.py'\n",
        "tpot.export(pipeline_filename)\n",
        "\n",
        "print(f\"âœ… Pipeline exported to: {pipeline_filename}\")\n",
        "\n",
        "# Download file (for Google Colab)\n",
        "from google.colab import files\n",
        "files.download(pipeline_filename)\n",
        "print(f\"ðŸ“¥ Downloaded: {pipeline_filename}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> ### Generate Summary Report"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create comprehensive summary report\n",
        "report = f\"\"\"\n",
        "{'='*70}\n",
        "TPOT CREDIT CARD FRAUD DETECTION - SUMMARY REPORT\n",
        "{'='*70}\n",
        "\n",
        "Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
        "\n",
        "DATASET INFORMATION\n",
        "{'-'*70}\n",
        "Total Samples:        {len(dataset):,}\n",
        "Training Samples:     {len(X_train_final):,}\n",
        "Test Samples:         {len(X_test):,}\n",
        "Number of Features:   {X_train.shape[1]}\n",
        "Fraud Cases (Train):  {sum(y_train_final)} ({sum(y_train_final)/len(y_train_final)*100:.2f}%)\n",
        "Fraud Cases (Test):   {sum(y_test)} ({sum(y_test)/len(y_test)*100:.2f}%)\n",
        "SMOTE Applied:        {USE_SMOTE}\n",
        "\n",
        "TPOT CONFIGURATION\n",
        "{'-'*70}\n",
        "Generations:          {GENERATIONS}\n",
        "Population Size:      {POPULATION_SIZE}\n",
        "CV Folds:             {CV_FOLDS}\n",
        "Scoring Metric:       {SCORING}\n",
        "Training Time:        {training_duration:.2f} seconds ({training_duration/60:.2f} minutes)\n",
        "\n",
        "MODEL PERFORMANCE\n",
        "{'-'*70}\n",
        "Accuracy:             {accuracy:.4f}\n",
        "Precision:            {precision:.4f}\n",
        "Recall:               {recall:.4f}\n",
        "F1 Score:             {f1:.4f}\n",
        "ROC-AUC:              {roc_auc:.4f}\n",
        "\n",
        "CONFUSION MATRIX\n",
        "{'-'*70}\n",
        "True Negatives:       {cm[0,0]:,}\n",
        "False Positives:      {cm[0,1]:,}\n",
        "False Negatives:      {cm[1,0]:,}\n",
        "True Positives:       {cm[1,1]:,}\n",
        "\n",
        "BEST PIPELINE\n",
        "{'-'*70}\n",
        "{tpot.fitted_pipeline_}\n",
        "\n",
        "KEY INSIGHTS\n",
        "{'-'*70}\n",
        "âœ“ The model detected {cm[1,1]} out of {sum(y_test)} fraud cases\n",
        "âœ“ Missed {cm[1,0]} fraud cases (False Negatives)\n",
        "âœ“ Flagged {cm[0,1]} normal transactions as fraud (False Positives)\n",
        "âœ“ Overall detection rate: {recall*100:.1f}%\n",
        "\n",
        "{'='*70}\n",
        "\"\"\"\n",
        "\n",
        "print(report)\n",
        "\n",
        "# Save report to file\n",
        "report_filename = f'tpot_fraud_report_{datetime.now().strftime(\"%Y%m%d_%H%M%S\")}.txt'\n",
        "with open(report_filename, 'w') as f:\n",
        "    f.write(report)\n",
        "\n",
        "print(f\"\\nâœ… Report saved to: {report_filename}\")\n",
        "files.download(report_filename)\n",
        "print(f\"ðŸ“¥ Downloaded: {report_filename}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> ### Save Model (Optional)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save the trained model for later use\n",
        "import joblib\n",
        "\n",
        "model_filename = f'tpot_fraud_model_{datetime.now().strftime(\"%Y%m%d_%H%M%S\")}.pkl'\n",
        "joblib.dump(tpot.fitted_pipeline_, model_filename)\n",
        "\n",
        "print(f\"âœ… Model saved to: {model_filename}\")\n",
        "files.download(model_filename)\n",
        "print(f\"ðŸ“¥ Downloaded: {model_filename}\")\n",
        "\n",
        "print(\"\\nðŸ’¡ To load the model later:\")\n",
        "print(\"   loaded_model = joblib.load('model_filename.pkl')\")\n",
        "print(\"   predictions = loaded_model.predict(X_new)\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
