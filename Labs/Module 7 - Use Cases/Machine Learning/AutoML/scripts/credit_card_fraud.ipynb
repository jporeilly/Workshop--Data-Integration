{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q0iaOYCDIvu1"
      },
      "source": [
        "# TPOT - Credit Card Fraud"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7bR4IYms0VYr"
      },
      "source": [
        "> ### Install TPOT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3XuglahR0dlv"
      },
      "outputs": [],
      "source": [
        "# Installs TPOT libraries.\n",
        "!pip install tpot"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dn_t1SOrJwOC"
      },
      "source": [
        "> ### Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gBxSPYHsJ7Hj"
      },
      "outputs": [],
      "source": [
        "# import libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tpot import TPOTClassifier\n",
        "from sklearn import preprocessing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from dask.distributed import Client, LocalCluster"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rogF_vGbLjWx"
      },
      "source": [
        "> ### Import Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aHkRl7NdOYfs"
      },
      "outputs": [],
      "source": [
        "# access your local drive. Select:  ~/Workshop--Data-Integration/Labs/Module 6 - Machine-Learning/01 Credit Card/AutoML/data/TPOT.csv\n",
        "from google.colab import files\n",
        "uploaded = files.upload()\n",
        "for fn in uploaded.keys():\n",
        "  print('User uploaded file \"{name}\" with length {length} bytes'.format(\n",
        "      name=fn, length=len(uploaded[fn])))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7AqVkigoO7Yc"
      },
      "outputs": [],
      "source": [
        "# Load the TPOT.csv dataset - Headless\n",
        "dataset = pd.read_csv('TPOT.csv', sep= ';', header=None)\n",
        "x = dataset.iloc[:, :-1].values\n",
        "y = dataset.iloc[:, 8].values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v9wQTM_6Mo-T"
      },
      "outputs": [],
      "source": [
        "# backup option pulls dataset from GitHub repository.\n",
        "# url = 'https://github.com/jporeilly/How-To--Machine-Learning/blob/main/data/TPOT.csv'\n",
        "# dataset = pd.read_csv(url, sep= ';', header=None)\n",
        "# x = dataset.iloc[:, :-1].values\n",
        "# y = dataset.iloc[:, 8].values"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3NURFI_GMUHl"
      },
      "source": [
        "> #### Data Exploration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VuTFh1I8M25U"
      },
      "outputs": [],
      "source": [
        "# displays dataset and outputs independent x variables and dependent y variable\n",
        "dataset.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d2Ewbaeg2pVb"
      },
      "outputs": [],
      "source": [
        "print (x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DrOKwScg2v6T"
      },
      "outputs": [],
      "source": [
        "print (y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B1JPT2dcMaDa"
      },
      "source": [
        "> ### Add Column Headers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ht3fAR28lzIh"
      },
      "outputs": [],
      "source": [
        "# adds column headers\n",
        "dataset.columns = ['first_time_customer','order_dollar_amount','num_items','age','web_order','total_transactions_to_date','hour_of_day','billing_shipping_zip_equal','reported_as_fraud_historic']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-jlCp13jmzie"
      },
      "source": [
        "> #### Check Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xzIanu7gm5co"
      },
      "outputs": [],
      "source": [
        "# check column headers\n",
        "dataset.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ov1Z26Nr5CpZ"
      },
      "source": [
        "> ### Convert Dataset to Numpy Array and Fit (optional)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K2PYKxTj5GmS"
      },
      "outputs": [],
      "source": [
        "# convert to numpy array and fit data\n",
        "x = dataset.iloc[:,0:-1].values\n",
        "min_max_scaler = preprocessing.MinMaxScaler()\n",
        "x_scaled = min_max_scaler.fit_transform(x)\n",
        "X=np.asarray(x_scaled)\n",
        "y=np.asarray(dataset.iloc[:,-1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b-t0NFd36z2S"
      },
      "source": [
        "> #### Check Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pr0nunc_7YPS"
      },
      "outputs": [],
      "source": [
        "dataset.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TAXfHB901vhj"
      },
      "source": [
        "> ### Splitting the Dataset: Train / Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NMpR33la6nVk"
      },
      "outputs": [],
      "source": [
        "# split the dataset 75% used for test\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.75, random_state=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IxXkBXkK7Scu"
      },
      "source": [
        "> ### TPOT Classifier - Option 1: With Dask (Recommended for Large Datasets)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "61rEjbvn7xB6"
      },
      "outputs": [],
      "source": [
        "# TPOT Classifier with Dask distributed computing\n",
        "# Set up a local Dask cluster and client\n",
        "cluster = LocalCluster(\n",
        "    n_workers=2,           # Number of worker processes\n",
        "    threads_per_worker=2,  # Number of threads per worker\n",
        "    memory_limit='2GB'     # Memory limit per worker\n",
        ")\n",
        "client = Client(cluster)\n",
        "\n",
        "try:\n",
        "    # Initialize TPOT with Dask support\n",
        "    tpot = TPOTClassifier(\n",
        "        generations=5,          # Number of iterations to run\n",
        "        population_size=20,     # Number of models to evaluate per generation\n",
        "        use_dask=True,          # Enable Dask distributed computing\n",
        "        verbosity=2,            # Display progress information\n",
        "        random_state=42,        # For reproducibility\n",
        "        cv=5,                   # Cross-validation folds\n",
        "        n_jobs=1                # Jobs per worker (use 1 with Dask)\n",
        "    )\n",
        "    \n",
        "    # Fit the model\n",
        "    print(\"Training TPOT model...\")\n",
        "    tpot.fit(X_train, y_train)\n",
        "    \n",
        "    # Evaluate on test set\n",
        "    test_score = tpot.score(X_test, y_test)\n",
        "    print(f\"\\nTest Set Accuracy: {test_score:.4f}\")\n",
        "    print(f\"\\nBest Pipeline:\\n{tpot.fitted_pipeline_}\")\n",
        "    \n",
        "finally:\n",
        "    # Clean up Dask resources\n",
        "    client.close()\n",
        "    cluster.close()\n",
        "    print(\"\\nDask cluster closed.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> ### TPOT Classifier - Option 2: Without Dask (Simpler, Good for Smaller Datasets)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TPOT Classifier without Dask - simpler approach\n",
        "# This runs faster on single machines and smaller datasets\n",
        "\n",
        "tpot_simple = TPOTClassifier(\n",
        "    generations=5,          # Number of iterations to run\n",
        "    population_size=20,     # Number of models to evaluate per generation\n",
        "    n_jobs=-1,              # Use all available CPU cores\n",
        "    verbosity=2,            # Display progress information\n",
        "    random_state=42,        # For reproducibility\n",
        "    cv=5                    # Cross-validation folds\n",
        ")\n",
        "\n",
        "# Fit the model\n",
        "print(\"Training TPOT model...\")\n",
        "tpot_simple.fit(X_train, y_train)\n",
        "\n",
        "# Evaluate on test set\n",
        "test_score = tpot_simple.score(X_test, y_test)\n",
        "print(f\"\\nTest Set Accuracy: {test_score:.4f}\")\n",
        "print(f\"\\nBest Pipeline:\\n{tpot_simple.fitted_pipeline_}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6jAicg4DB-PD"
      },
      "source": [
        "> #### Export Pipeline as Python script"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vpHMD0nOCfhn"
      },
      "outputs": [],
      "source": [
        "# export results to python\n",
        "# Use tpot if you ran Option 1 (with Dask), or tpot_simple if you ran Option 2\n",
        "tpot.export('tpot_exported_credit_card_pipeline.py')\n",
        "from google.colab import files\n",
        "files.download('tpot_exported_credit_card_pipeline.py')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wNKcBUXd8NxX"
      },
      "source": [
        ">#### TPOT Evaluated Pipelines"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xcLhep898S_B"
      },
      "outputs": [],
      "source": [
        "# output as JSON each pipeline\n",
        "# Use tpot if you ran Option 1 (with Dask), or tpot_simple if you ran Option 2\n",
        "print(tpot.evaluated_individuals_)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
